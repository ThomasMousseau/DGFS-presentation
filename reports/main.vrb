\frametitle{}
\footnotesize

\vspace{-0.2cm}

\begin{algorithm}[H]
    \footnotesize
\caption{DGFS Training}
\label{alg:dgfs}
\begin{algorithmic}[1]
\Require $\mu(\cdot)$, $\bar{\sigma}$, $N$, $\lambda$, $B$, $\eta$
\State Init $\theta=(\theta_f,\phi)$
\Repeat
    \State Sample trajectories:
    \For{$b=1$ to $B$}
        \State $\tau^{(b)}=(x^{(b)}_0,\ldots,x^{(b)}_N)$ under $x_{n+1}=x_n+h f_{\theta_f}(x_n,n)+\sqrt{h}\bar{\sigma}\varepsilon_n$, $\varepsilon_n\sim\mathcal N(0,I)$
    \EndFor
    \State Build subtrajectories: $\mathcal S(\tau^{(b)})$ of $(m,n)$ with $0\le m<n\le N$
    \State Compute SubTB loss:
    \begin{equation*}
    \mathcal L(\tau^{(b)};\theta)=\sum_{(m,n)\in\mathcal S(\tau^{(b)})}\lambda^{n-m}\left[\log\frac{F_\phi(x_m)\prod_{l=m}^{n-1} P_F(x_{l+1}\mid x_l;\theta_f)}{F_\phi(x_n)\prod_{l=m}^{n-1} P_B^{\mathrm{ref}}(x_l\mid x_{l+1})}\right]^2
    \end{equation*}
    \State $g \leftarrow \nabla_{\theta}\frac{1}{B}\sum_{b=1}^B \mathcal L(\tau^{(b)};\theta)$
    \State $\theta \leftarrow \mathrm{Adam}(\theta, g, \eta)$
\Until{convergence}
\end{algorithmic}
\end{algorithm}
